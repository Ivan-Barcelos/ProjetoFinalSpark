{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init SparkSession with HiveSupport\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Hive Connection\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from csv in hdfs\n",
    "covid_df = spark.read.csv('/user/ivan/final_spark_project/covid_br_data/*.csv', sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regiao', 'string'),\n",
       " ('estado', 'string'),\n",
       " ('municipio', 'string'),\n",
       " ('coduf', 'string'),\n",
       " ('codmun', 'string'),\n",
       " ('codRegiaoSaude', 'string'),\n",
       " ('nomeRegiaoSaude', 'string'),\n",
       " ('data', 'string'),\n",
       " ('semanaEpi', 'string'),\n",
       " ('populacaoTCU2019', 'string'),\n",
       " ('casosAcumulado', 'string'),\n",
       " ('casosNovos', 'string'),\n",
       " ('obitosAcumulado', 'string'),\n",
       " ('obitosNovos', 'string'),\n",
       " ('Recuperadosnovos', 'string'),\n",
       " ('emAcompanhamentoNovos', 'string'),\n",
       " ('interior/metropolitana', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataframe Schema\n",
    "covid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-25 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 0          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe data\n",
    "covid_df.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe Schema acording with the data\n",
    "covid_df_1 = covid_df.select(\"regiao\",\n",
    "                             \"estado\",\n",
    "                             \"municipio\",\n",
    "                             col(\"coduf\").cast(\"integer\"),\n",
    "                             col(\"codmun\").cast(\"integer\"),\n",
    "                             col(\"codRegiaoSaude\").cast(\"integer\"),\n",
    "                             \"nomeRegiaoSaude\",\n",
    "                             col(\"data\").cast(\"date\"),\n",
    "                             col(\"semanaEpi\").cast(\"integer\"),\n",
    "                             col(\"populacaoTCU2019\").cast(\"date\"),\n",
    "                             col(\"casosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"casosNovos\").cast(\"integer\"),\n",
    "                             col(\"obitosAcumulado\").cast(\"string\"),\n",
    "                             col(\"obitosNovos\").cast(\"integer\"),\n",
    "                             col(\"Recuperadosnovos\").cast(\"integer\"),\n",
    "                             col(\"emAcompanhamentoNovos\").cast(\"integer\"),\n",
    "                             col(\"interior/metropolitana\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Hive Database\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the data from HDFS To Hive\n",
    "covid_df_1.write.format(\"csv\").partitionBy(\"municipio\").saveAsTable(\"Covid_br_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "| default|acompanhamento|      false|\n",
      "| default| covid_br_data|      false|\n",
      "| default|         juros|      false|\n",
      "| default|   recuperados|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table created\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Recuperados = spark.sql(\"select Recuperadosnovos as Casos_Recuperados from covid_br_data order by 1 desc limit 1\")\n",
    "Recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Em_Acompanhamento|\n",
      "+-----------------+\n",
      "|          1317658|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Acompanhamento = spark.sql(\"select emAcompanhamentoNovos as Em_Acompanhamento from covid_br_data order by 1 desc limit 1\")\n",
    "Acompanhamento.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosAcumulado = spark.sql(\"select casosAcumulado as Acumulado from covid_br_data order by 1 desc limit 1\")\n",
    "casosAcumulado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Casos_Novos|\n",
      "+-----------+\n",
      "|     115228|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosNovos = spark.sql(\"select casosNovos as Casos_Novos from covid_br_data order by 1 desc limit 1\")\n",
    "casosNovos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Incidencia|\n",
      "+-----------------+\n",
      "|8972.292625940041|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Incidencia = spark.sql(\"select ((casosAcumulado/210147125)*100000) as Incidencia from covid_br_data order by 1 desc limit 1\")\n",
    "Incidencia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "|526892|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kafka only accept data from a string type value column \n",
    "# Change the current result for a string type named value column \n",
    "Obitos_Acumulados = spark.sql(\"select obitosAcumulado from covid_br_data order by 1 desc limit 1\")\n",
    "Obitos_Acumulados_string = Obitos_Acumulados.withColumn(\"value\", col(\"obitosAcumulado\").cast(StringType())).drop(\"obitosAcumulado\")\n",
    "Obitos_Acumulados_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| 4249|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obitosNovos = spark.sql(\"select obitosNovos from covid_br_data order by 1 desc limit 1\")\n",
    "obitosNovos_string = obitosNovos.withColumn(\"value\", col(\"obitosNovos\").cast(StringType())).drop(\"obitosNovos\")\n",
    "obitosNovos_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             value|\n",
      "+------------------+\n",
      "|250.72529543290204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mortalidade = spark.sql(\"select ((obitosAcumulado/210147125)*100000) as Mortalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Mortalidade_string = Mortalidade.withColumn(\"value\", col(\"Mortalidade\").cast(StringType())).drop(\"Mortalidade\")\n",
    "Mortalidade_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|4.903522134515072|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Letalidade = spark.sql(\"select obitosNovos, casosNovos, (obitosNovos/casosNovos)*100 as Letalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Letalidade_string = Letalidade.withColumn(\"value\", col(\"Letalidade\").cast(StringType())).drop(\"obitosNovos\", \"casosNovos\", \"Letalidade\")\n",
    "Letalidade_string.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recuperados.write.format(\"csv\").saveAsTable(\"Recuperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento.write.format(\"csv\").saveAsTable(\"Acompanhamento\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to HDFS as parquet with snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosAcumulado.write.option(\"compression\",\"snappy\").parquet(\"/user/ivan/final_spark_project/casosAcumulado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosNovos.write.option(\"compression\",\"snappy\").parquet(\"/user/ivan/final_spark_project/casosNovos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "Incidencia.write.option(\"compression\",\"snappy\").parquet(\"/user/ivan/final_spark_project/Incidencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "Obitos_Acumulados_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\",\"topic-Obitos_Acumulados\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitosNovos_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Obitos_Novos\") \\\n",
    "                .option(\"checkpointLocation\",\"user/ivan/kafka_checkpoint_Obitos_Novos\")\\\n",
    "                .option(\"path\",\"hdfs://namenode:50070/user/ivan/kafka/topic-Obitos_Novos\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortalidade_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Mortalidade\") \\\n",
    "                .option(\"checkpointLocation\",\"user/ivan/kafka_checkpoint_Mortalidade\")\\\n",
    "                .option(\"path\",\"hdfs://namenode:50070/user/ivan/kafka/topic-Mortalidade\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letalidade_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Letalidade\") \\\n",
    "                .option(\"checkpointLocation\",\"user/ivan/kafka_checkpoint_Letalidade\")\\\n",
    "                .option(\"path\",\"hdfs://namenode:50070/user/ivan/kafka/topic-Letalidade\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      regiao|   Casos|Obitos|Incidencia|Mortalidade|Atualizacao|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      Brasil|18855015|526892|   8972.29|     250.73| 2021-07-06|\n",
      "|Centro-Oeste|  686433| 19485|    326.64|       9.27| 2021-07-06|\n",
      "|    Nordeste| 1141612| 24428|    543.24|      11.62| 2021-07-06|\n",
      "|       Norte|  557708| 15624|    265.39|       7.43| 2021-07-06|\n",
      "|     Sudeste| 3809222|130389|   1812.65|      62.05| 2021-07-06|\n",
      "|         Sul| 1308643| 31867|    622.73|      15.16| 2021-07-06|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"select regiao, \\\n",
    "         max(casosAcumulado) as Casos, \\\n",
    "         max(obitosAcumulado) as Obitos, \\\n",
    "         max(cast(((casosAcumulado/210147125)*100000) as decimal(18,2))) as Incidencia, \\\n",
    "         max(cast(((obitosAcumulado/210147125)*100000) as decimal(18,2))) as Mortalidade , \\\n",
    "         max(data) as Atualizacao\\\n",
    "         from covid_br_data \\\n",
    "         group by regiao \\\n",
    "         order by regiao\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
