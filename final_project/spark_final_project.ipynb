{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init SparkSession with HiveSupport\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Hive Connection\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move csv files to HDFS\n",
    "!hdfs dfs -mkdir -p /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from csv in hdfs\n",
    "covid_df = spark.read.csv('/tmp/final_spark_project/covid_br_data/*.csv', sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regiao', 'string'),\n",
       " ('estado', 'string'),\n",
       " ('municipio', 'string'),\n",
       " ('coduf', 'string'),\n",
       " ('codmun', 'string'),\n",
       " ('codRegiaoSaude', 'string'),\n",
       " ('nomeRegiaoSaude', 'string'),\n",
       " ('data', 'string'),\n",
       " ('semanaEpi', 'string'),\n",
       " ('populacaoTCU2019', 'string'),\n",
       " ('casosAcumulado', 'string'),\n",
       " ('casosNovos', 'string'),\n",
       " ('obitosAcumulado', 'string'),\n",
       " ('obitosNovos', 'string'),\n",
       " ('Recuperadosnovos', 'string'),\n",
       " ('emAcompanhamentoNovos', 'string'),\n",
       " ('interior/metropolitana', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataframe Schema\n",
    "covid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-25 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 0          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe data\n",
    "covid_df.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe Schema acording with the data\n",
    "covid_df_1 = covid_df.select(\"regiao\",\n",
    "                             \"estado\",\n",
    "                             \"municipio\",\n",
    "                             col(\"coduf\").cast(\"integer\"),\n",
    "                             col(\"codmun\").cast(\"integer\"),\n",
    "                             col(\"codRegiaoSaude\").cast(\"integer\"),\n",
    "                             \"nomeRegiaoSaude\",\n",
    "                             col(\"data\").cast(\"date\"),\n",
    "                             col(\"semanaEpi\").cast(\"integer\"),\n",
    "                             col(\"populacaoTCU2019\").cast(\"date\"),\n",
    "                             col(\"casosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"casosNovos\").cast(\"integer\"),\n",
    "                             col(\"obitosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"obitosNovos\").cast(\"integer\"),\n",
    "                             col(\"Recuperadosnovos\").cast(\"integer\"),\n",
    "                             col(\"emAcompanhamentoNovos\").cast(\"integer\"),\n",
    "                             col(\"interior/metropolitana\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Hive Database\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the data from HDFS To Hive\n",
    "covid_df_1.write.format(\"csv\").partitionBy(\"municipio\").saveAsTable(\"covid_br_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|covid_br_data|      false|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table created\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First View - Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Recuperados = spark.sql(\"select Recuperadosnovos as Casos_Recuperados from covid_br_data order by 1 desc limit 1\")\n",
    "Recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento = spark.sql(\"select emAcompanhamentoNovos as Em_Acompanhamento from covid_br_data order by 1 desc limit 1\")\n",
    "Acompanhamento.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second View - HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosAcumulado = spark.sql(\"select casosAcumulado as Acumulado from covid_br_data order by 1 desc limit 1\")\n",
    "casosAcumulado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Casos_Novos|\n",
      "+-----------+\n",
      "|     115228|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosNovos = spark.sql(\"select casosNovos as Casos_Novos from covid_br_data order by 1 desc limit 1\")\n",
    "casosNovos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Incidencia|\n",
      "+-----------------+\n",
      "|8972.292625940041|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Incidencia = spark.sql(\"select ((casosAcumulado/210147125)*100000) as Incidencia from covid_br_data order by 1 desc limit 1\")\n",
    "Incidencia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third View - Kakfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[obitosAcumulado: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Kafka only accept data from a string type value column \n",
    "# Change the current result for a string type named value column \n",
    "Obitos_Acumulados = spark.sql(\"select obitosAcumulado from covid_br_data order by 1 desc limit 1\")\n",
    "Obitos_Acumulados_string = Obitos_Acumulados.withColumn(\"value\", col(\"obitosAcumulado\").cast(StringType())).drop(\"obitosAcumulado\")\n",
    "Obitos_Acumulados_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "| 4249|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obitosNovos = spark.sql(\"select obitosNovos from covid_br_data order by 1 desc limit 1\")\n",
    "obitosNovos_string = obitosNovos.withColumn(\"value\", col(\"obitosNovos\").cast(StringType())).drop(\"obitosNovos\")\n",
    "obitosNovos_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|             value|\n",
      "+------------------+\n",
      "|250.72529543290204|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mortalidade = spark.sql(\"select ((obitosAcumulado/210147125)*100000) as Mortalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Mortalidade_string = Mortalidade.withColumn(\"value\", col(\"Mortalidade\").cast(StringType())).drop(\"Mortalidade\")\n",
    "Mortalidade_string.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|4.903522134515072|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Letalidade = spark.sql(\"select obitosNovos, casosNovos, (obitosNovos/casosNovos)*100 as Letalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Letalidade_string = Letalidade.withColumn(\"value\", col(\"Letalidade\").cast(StringType())).drop(\"obitosNovos\", \"casosNovos\", \"Letalidade\")\n",
    "Letalidade_string.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recuperados.write.format(\"csv\").saveAsTable(\"Recuperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento.write.format(\"csv\").saveAsTable(\"Acompanhamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "| default|acompanhamento|      false|\n",
      "| default| covid_br_data|      false|\n",
      "| default|   recuperados|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the created tables\n",
    "spark.sql(\"Show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to HDFS as parquet with snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosAcumulado.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosAcumulado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosNovos.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosNovos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Incidencia.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/Incidencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:52 /user/final_spark_project/Incidencia\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:42 /user/final_spark_project/casosAcumulado\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:47 /user/final_spark_project/casosNovos\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 18:08 /user/final_spark_project/covid_br_data\r\n"
     ]
    }
   ],
   "source": [
    "# Check the files in HDFS\n",
    "!hdfs dfs -ls /user/final_spark_project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Obitos_Acumulados_string.write\n",
    "                .format(\"kafka\") \n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                .option(\"topic\",\"topic-Obitos_Acumulados\")                \n",
    "                .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (spark \n",
    "        .read \n",
    "        .format(\"kafka\") \n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "        .option(\"subscribe\", \"topic-Obitos_Acumulados\")  \n",
    "        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+--------------------+---------+------+--------------------+-------------+\n",
      "| key|           value|               topic|partition|offset|           timestamp|timestampType|\n",
      "+----+----------------+--------------------+---------+------+--------------------+-------------+\n",
      "|null|[39 39 39 38 39]|topic-Obitos_Acum...|        0|     0|2021-11-10 00:06:...|            0|\n",
      "|null|[39 39 39 38 39]|topic-Obitos_Acum...|        0|     1|2021-11-10 00:12:...|            0|\n",
      "|null|[39 39 39 38 39]|topic-Obitos_Acum...|        0|     2|2021-11-10 00:22:...|            0|\n",
      "+----+----------------+--------------------+---------+------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitosNovos_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Obitos_Novos\") \\\n",
    "                .option(\"checkpointLocation\",\"user/kafka_checkpoint_Obitos_Novos\")\\\n",
    "                .option(\"path\",\"hdfs:///user/kafka/topic-Obitos_Novos\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortalidade_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Mortalidade\") \\\n",
    "                .option(\"checkpointLocation\",\"user/kafka_checkpoint_Mortalidade\")\\\n",
    "                .option(\"path\",\"hdfs:///user/kafka/topic-Mortalidade\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letalidade_string.write\\\n",
    "                .format(\"kafka\") \\\n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \\\n",
    "                .option(\"topic\", \"Letalidade\") \\\n",
    "                .option(\"checkpointLocation\",\"user/kafka_checkpoint_Letalidade\")\\\n",
    "                .option(\"path\",\"hdfs:///user/kafka/topic-Letalidade\") \\\n",
    "                .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      regiao|   Casos|Obitos|Incidencia|Mortalidade|Atualizacao|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      Brasil|18855015| 99572|   8972.29|     250.73| 2021-07-06|\n",
      "|Centro-Oeste|  686433|  9980|    326.64|       9.27| 2021-07-06|\n",
      "|    Nordeste| 1141612|  9993|    543.24|      11.62| 2021-07-06|\n",
      "|       Norte|  557708|  9992|    265.39|       7.43| 2021-07-06|\n",
      "|     Sudeste| 3809222| 99989|   1812.65|      62.05| 2021-07-06|\n",
      "|         Sul| 1308643|   999|    622.73|      15.16| 2021-07-06|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"\"\"select regiao, \n",
    "         max(casosAcumulado) as Casos, \n",
    "         max(obitosAcumulado) as Obitos, \n",
    "         max(cast(((casosAcumulado/210147125)*100000) as decimal(18,2))) as Incidencia, \n",
    "         max(cast(((obitosAcumulado/210147125)*100000) as decimal(18,2))) as Mortalidade, \n",
    "         max(data) as Atualizacao\n",
    "         from covid_br_data \n",
    "         group by regiao \n",
    "         order by regiao\"\"\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic in elastic\n",
    "# Dictionary\n",
    "esconf={}\n",
    "esconf[\"es.mapping.id\"] = \"_id\"\n",
    "esconf[\"es.nodes\"] = \"localhost\"\n",
    "esconf[\"es.port\"] = \"9200\"\n",
    "esconf[\"es.update.script.inline\"] = \"ctx._source.location = params.location\"\n",
    "esconf[\"es.update.script.params\"] = \"location:\"\n",
    "esconf[\"es.write.operation\"] = \"upsert\"\n",
    "\n",
    "# Dataframe to Elastic - topic\n",
    "Obitos_Acumulados.write \\\n",
    "                 .format(\"org.elasticsearch.spark.sql\") \\\n",
    "                 .options(**esconf) \\\n",
    "                 .save(\"/user/elastic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
