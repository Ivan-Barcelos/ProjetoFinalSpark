{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init SparkSession with HiveSupport\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Hive Connection\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move csv files to HDFS\n",
    "!hdfs dfs -mkdir -p /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from csv in hdfs\n",
    "covid_df = spark.read.csv('/tmp/final_spark_project/covid_br_data/*.csv', sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regiao', 'string'),\n",
       " ('estado', 'string'),\n",
       " ('municipio', 'string'),\n",
       " ('coduf', 'string'),\n",
       " ('codmun', 'string'),\n",
       " ('codRegiaoSaude', 'string'),\n",
       " ('nomeRegiaoSaude', 'string'),\n",
       " ('data', 'string'),\n",
       " ('semanaEpi', 'string'),\n",
       " ('populacaoTCU2019', 'string'),\n",
       " ('casosAcumulado', 'string'),\n",
       " ('casosNovos', 'string'),\n",
       " ('obitosAcumulado', 'string'),\n",
       " ('obitosNovos', 'string'),\n",
       " ('Recuperadosnovos', 'string'),\n",
       " ('emAcompanhamentoNovos', 'string'),\n",
       " ('interior/metropolitana', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataframe Schema\n",
    "covid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-25 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 0          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe data\n",
    "covid_df.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe Schema acording with the data\n",
    "covid_df_1 = covid_df.select(\"regiao\",\n",
    "                             \"estado\",\n",
    "                             \"municipio\",\n",
    "                             col(\"coduf\").cast(\"integer\"),\n",
    "                             col(\"codmun\").cast(\"integer\"),\n",
    "                             col(\"codRegiaoSaude\").cast(\"integer\"),\n",
    "                             \"nomeRegiaoSaude\",\n",
    "                             col(\"data\").cast(\"date\"),\n",
    "                             col(\"semanaEpi\").cast(\"integer\"),\n",
    "                             col(\"populacaoTCU2019\").cast(\"date\"),\n",
    "                             col(\"casosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"casosNovos\").cast(\"integer\"),\n",
    "                             col(\"obitosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"obitosNovos\").cast(\"integer\"),\n",
    "                             col(\"Recuperadosnovos\").cast(\"integer\"),\n",
    "                             col(\"emAcompanhamentoNovos\").cast(\"integer\"),\n",
    "                             col(\"interior/metropolitana\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Hive Database\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the data from HDFS To Hive\n",
    "covid_df_1.write.format(\"csv\").partitionBy(\"municipio\").saveAsTable(\"covid_br_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|covid_br_data|      false|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table created\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First View - Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Recuperados = spark.sql(\"select Recuperadosnovos as Casos_Recuperados from covid_br_data order by 1 desc limit 1\")\n",
    "Recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento = spark.sql(\"select emAcompanhamentoNovos as Em_Acompanhamento from covid_br_data order by 1 desc limit 1\")\n",
    "Acompanhamento.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second View - HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosAcumulado = spark.sql(\"select casosAcumulado as Acumulado from covid_br_data order by 1 desc limit 1\")\n",
    "casosAcumulado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Casos_Novos|\n",
      "+-----------+\n",
      "|     115228|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosNovos = spark.sql(\"select casosNovos as Casos_Novos from covid_br_data order by 1 desc limit 1\")\n",
    "casosNovos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Incidencia|\n",
      "+-----------------+\n",
      "|8972.292625940041|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Incidencia = spark.sql(\"select ((casosAcumulado/210147125)*100000) as Incidencia from covid_br_data order by 1 desc limit 1\")\n",
    "Incidencia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third View - Kakfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka only accept data from a string type value column \n",
    "# Put the dataframe result into a json format to further use\n",
    "Obitos_Acumulados = spark.sql(\"select obitosAcumulado from covid_br_data order by 1 desc limit 1\")\n",
    "Obitos_Acumulados_json = Obitos_Acumulados.select(to_json(struct(\"obitosAcumulado\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|value                     |\n",
      "+--------------------------+\n",
      "|{\"obitosAcumulado\":526892}|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Obitos_Acumulados_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitosNovos = spark.sql(\"select obitosNovos from covid_br_data order by 1 desc limit 1\")\n",
    "obitosNovos_json = obitosNovos.select(to_json(struct(\"obitosNovos\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"obitosNovos\":4249}|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obitosNovos_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortalidade = spark.sql(\"select cast(((obitosAcumulado/210147125)*100000) as decimal) as Mortalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Mortalidade_json = Mortalidade.select(to_json(struct(\"Mortalidade\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|value              |\n",
      "+-------------------+\n",
      "|{\"Mortalidade\":251}|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mortalidade_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letalidade = spark.sql(\"select obitosNovos, casosNovos, cast((obitosNovos/casosNovos)*100 as decimal) as Letalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Letalidade_json = Letalidade.select(to_json(struct(\"Letalidade\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|value           |\n",
      "+----------------+\n",
      "|{\"Letalidade\":5}|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Letalidade_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recuperados.write.format(\"csv\").saveAsTable(\"Recuperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento.write.format(\"csv\").saveAsTable(\"Acompanhamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "| default|acompanhamento|      false|\n",
      "| default| covid_br_data|      false|\n",
      "| default|   recuperados|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the created tables\n",
    "spark.sql(\"Show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to HDFS as parquet with snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosAcumulado.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosAcumulado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosNovos.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosNovos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Incidencia.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/Incidencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:52 /user/final_spark_project/Incidencia\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:42 /user/final_spark_project/casosAcumulado\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:47 /user/final_spark_project/casosNovos\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 18:08 /user/final_spark_project/covid_br_data\r\n"
     ]
    }
   ],
   "source": [
    "# Check the files in HDFS\n",
    "!hdfs dfs -ls /user/final_spark_project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Obitos_Acumulados_json.write\n",
    "                .format(\"kafka\") \n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                .option(\"topic\",\"topic-obitos_acumulados\")                \n",
    "                .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Obitos_Acumulados_df = (spark \n",
    "                            .read \n",
    "                            .format(\"kafka\") \n",
    "                            .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                            .option(\"subscribe\", \"topic-obitos_acumulados\")  \n",
    "                            .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                            [526892]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"obitosAcumulado\", IntegerType())\n",
    "Obitos_Acumulados_df.where(\"timestamp >= '2021-11-10 12:52'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(obitosNovos_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-obitos_novos\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "obitosNovos_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-obitos_novos\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                              [4249]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"obitosNovos\", IntegerType())\n",
    "obitosNovos_df.select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Mortalidade_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-mortalidade\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Mortalidade_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-mortalidade\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                               [251]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"Mortalidade\", IntegerType())\n",
    "Mortalidade_df.where(\"timestamp >= '2021-11-10 13:52'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Letalidade_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-Letalidade\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Letalidade_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-Letalidade\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                                 [5]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"Letalidade\", IntegerType())\n",
    "Letalidade_df.where(\"timestamp >= '2021-11-10 14:15'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      regiao|   Casos|Obitos|Incidencia|Mortalidade|Atualizacao|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      Brasil|18855015| 99572|   8972.29|     250.73| 2021-07-06|\n",
      "|Centro-Oeste|  686433|  9980|    326.64|       9.27| 2021-07-06|\n",
      "|    Nordeste| 1141612|  9993|    543.24|      11.62| 2021-07-06|\n",
      "|       Norte|  557708|  9992|    265.39|       7.43| 2021-07-06|\n",
      "|     Sudeste| 3809222| 99989|   1812.65|      62.05| 2021-07-06|\n",
      "|         Sul| 1308643|   999|    622.73|      15.16| 2021-07-06|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"\"\"select regiao, \n",
    "         max(casosAcumulado) as Casos, \n",
    "         max(obitosAcumulado) as Obitos, \n",
    "         max(cast(((casosAcumulado/210147125)*100000) as decimal(18,2))) as Incidencia, \n",
    "         max(cast(((obitosAcumulado/210147125)*100000) as decimal(18,2))) as Mortalidade, \n",
    "         max(data) as Atualizacao\n",
    "         from covid_br_data \n",
    "         group by regiao \n",
    "         order by regiao\"\"\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a topic in elastic\n",
    "# Dictionary\n",
    "esconf={}\n",
    "esconf[\"es.mapping.id\"] = \"_id\"\n",
    "esconf[\"es.nodes\"] = \"localhost\"\n",
    "esconf[\"es.port\"] = \"9200\"\n",
    "esconf[\"es.update.script.inline\"] = \"ctx._source.location = params.location\"\n",
    "esconf[\"es.update.script.params\"] = \"location:\"\n",
    "esconf[\"es.write.operation\"] = \"upsert\"\n",
    "\n",
    "# Dataframe to Elastic - topic\n",
    "Obitos_Acumulados.write \\\n",
    "                 .format(\"org.elasticsearch.spark.sql\") \\\n",
    "                 .options(**esconf) \\\n",
    "                 .save(\"/user/elastic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
