{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init SparkSession with HiveSupport\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark-Hive Connection\") \\\n",
    "    .config(\"spark.sql.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move csv files to HDFS\n",
    "!hdfs dfs -mkdir -p /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv /tmp/final_spark_project/covid_br_data/\n",
    "!hdfs dfs -put /mnt/notebooks/covid_br_data/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv /tmp/final_spark_project/covid_br_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from csv in hdfs\n",
    "covid_df = spark.read.csv('/tmp/final_spark_project/covid_br_data/*.csv', sep=\";\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regiao', 'string'),\n",
       " ('estado', 'string'),\n",
       " ('municipio', 'string'),\n",
       " ('coduf', 'string'),\n",
       " ('codmun', 'string'),\n",
       " ('codRegiaoSaude', 'string'),\n",
       " ('nomeRegiaoSaude', 'string'),\n",
       " ('data', 'string'),\n",
       " ('semanaEpi', 'string'),\n",
       " ('populacaoTCU2019', 'string'),\n",
       " ('casosAcumulado', 'string'),\n",
       " ('casosNovos', 'string'),\n",
       " ('obitosAcumulado', 'string'),\n",
       " ('obitosNovos', 'string'),\n",
       " ('Recuperadosnovos', 'string'),\n",
       " ('emAcompanhamentoNovos', 'string'),\n",
       " ('interior/metropolitana', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check dataframe Schema\n",
    "covid_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------------\n",
      " regiao                 | Brasil     \n",
      " estado                 | null       \n",
      " municipio              | null       \n",
      " coduf                  | 76         \n",
      " codmun                 | null       \n",
      " codRegiaoSaude         | null       \n",
      " nomeRegiaoSaude        | null       \n",
      " data                   | 2020-02-25 \n",
      " semanaEpi              | 9          \n",
      " populacaoTCU2019       | 210147125  \n",
      " casosAcumulado         | 0          \n",
      " casosNovos             | 0          \n",
      " obitosAcumulado        | 0          \n",
      " obitosNovos            | 0          \n",
      " Recuperadosnovos       | null       \n",
      " emAcompanhamentoNovos  | null       \n",
      " interior/metropolitana | null       \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe data\n",
    "covid_df.show(1,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dataframe Schema acording with the data\n",
    "covid_df_1 = covid_df.select(\"regiao\",\n",
    "                             \"estado\",\n",
    "                             \"municipio\",\n",
    "                             col(\"coduf\").cast(\"integer\"),\n",
    "                             col(\"codmun\").cast(\"integer\"),\n",
    "                             col(\"codRegiaoSaude\").cast(\"integer\"),\n",
    "                             \"nomeRegiaoSaude\",\n",
    "                             col(\"data\").cast(\"date\"),\n",
    "                             col(\"semanaEpi\").cast(\"integer\"),\n",
    "                             col(\"populacaoTCU2019\").cast(\"date\"),\n",
    "                             col(\"casosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"casosNovos\").cast(\"integer\"),\n",
    "                             col(\"obitosAcumulado\").cast(\"integer\"),\n",
    "                             col(\"obitosNovos\").cast(\"integer\"),\n",
    "                             col(\"Recuperadosnovos\").cast(\"integer\"),\n",
    "                             col(\"emAcompanhamentoNovos\").cast(\"integer\"),\n",
    "                             col(\"interior/metropolitana\").cast(\"integer\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|databaseName|\n",
      "+------------+\n",
      "|     default|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the Hive Database\n",
    "spark.sql(\"show databases\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the data from HDFS To Hive\n",
    "covid_df_1.write.format(\"csv\").partitionBy(\"municipio\").saveAsTable(\"covid_br_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+-----------+\n",
      "|database|    tableName|isTemporary|\n",
      "+--------+-------------+-----------+\n",
      "| default|covid_br_data|      false|\n",
      "+--------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the table created\n",
    "spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframes for Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First View - Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|Casos_Recuperados|\n",
      "+-----------------+\n",
      "|         17262646|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Recuperados = spark.sql(\"select Recuperadosnovos as Casos_Recuperados from covid_br_data order by 1 desc limit 1\")\n",
    "Recuperados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento = spark.sql(\"select emAcompanhamentoNovos as Em_Acompanhamento from covid_br_data order by 1 desc limit 1\")\n",
    "Acompanhamento.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second View - HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Acumulado|\n",
      "+---------+\n",
      "| 18855015|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosAcumulado = spark.sql(\"select casosAcumulado as Acumulado from covid_br_data order by 1 desc limit 1\")\n",
    "casosAcumulado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Casos_Novos|\n",
      "+-----------+\n",
      "|     115228|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "casosNovos = spark.sql(\"select casosNovos as Casos_Novos from covid_br_data order by 1 desc limit 1\")\n",
    "casosNovos.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|       Incidencia|\n",
      "+-----------------+\n",
      "|8972.292625940041|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Incidencia = spark.sql(\"select ((casosAcumulado/210147125)*100000) as Incidencia from covid_br_data order by 1 desc limit 1\")\n",
    "Incidencia.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third View - Kakfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka only accept data from a string type value column \n",
    "# Put the dataframe result into a json format to further use\n",
    "Obitos_Acumulados = spark.sql(\"select obitosAcumulado from covid_br_data order by 1 desc limit 1\")\n",
    "Obitos_Acumulados_json = Obitos_Acumulados.select(to_json(struct(\"obitosAcumulado\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|value                     |\n",
      "+--------------------------+\n",
      "|{\"obitosAcumulado\":526892}|\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Obitos_Acumulados_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "obitosNovos = spark.sql(\"select obitosNovos from covid_br_data order by 1 desc limit 1\")\n",
    "obitosNovos_json = obitosNovos.select(to_json(struct(\"obitosNovos\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|{\"obitosNovos\":4249}|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "obitosNovos_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mortalidade = spark.sql(\"select cast(((obitosAcumulado/210147125)*100000) as decimal) as Mortalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Mortalidade_json = Mortalidade.select(to_json(struct(\"Mortalidade\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|value              |\n",
      "+-------------------+\n",
      "|{\"Mortalidade\":251}|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Mortalidade_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letalidade = spark.sql(\"select obitosNovos, casosNovos, cast((obitosNovos/casosNovos)*100 as decimal) as Letalidade from covid_br_data order by 1 desc limit 1\")\n",
    "Letalidade_json = Letalidade.select(to_json(struct(\"Letalidade\")).alias(\"value\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|value           |\n",
      "+----------------+\n",
      "|{\"Letalidade\":5}|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Letalidade_json.show(1,False,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Hive Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recuperados.write.format(\"csv\").saveAsTable(\"Recuperados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Acompanhamento.write.format(\"csv\").saveAsTable(\"Acompanhamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-----------+\n",
      "|database|     tableName|isTemporary|\n",
      "+--------+--------------+-----------+\n",
      "| default|acompanhamento|      false|\n",
      "| default| covid_br_data|      false|\n",
      "| default|   recuperados|      false|\n",
      "+--------+--------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the created tables\n",
    "spark.sql(\"Show tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to HDFS as parquet with snappy compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosAcumulado.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosAcumulado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "casosNovos.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/casosNovos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Incidencia.write.option(\"compression\",\"snappy\").parquet(\"/user/final_spark_project/Incidencia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:52 /user/final_spark_project/Incidencia\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:42 /user/final_spark_project/casosAcumulado\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 19:47 /user/final_spark_project/casosNovos\r\n",
      "drwxr-xr-x   - root supergroup          0 2021-11-09 18:08 /user/final_spark_project/covid_br_data\r\n"
     ]
    }
   ],
   "source": [
    "# Check the files in HDFS\n",
    "!hdfs dfs -ls /user/final_spark_project/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to Kafka topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Obitos_Acumulados_json.write\n",
    "                .format(\"kafka\") \n",
    "                .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                .option(\"topic\",\"topic-obitos_acumulados\")                \n",
    "                .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Obitos_Acumulados_df = (spark \n",
    "                            .read \n",
    "                            .format(\"kafka\") \n",
    "                            .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                            .option(\"subscribe\", \"topic-obitos_acumulados\")  \n",
    "                            .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                            [526892]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"obitosAcumulado\", IntegerType())\n",
    "Obitos_Acumulados_df.where(\"timestamp >= '2021-11-10 12:52'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(obitosNovos_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-obitos_novos\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "obitosNovos_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-obitos_novos\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                              [4249]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"obitosNovos\", IntegerType())\n",
    "obitosNovos_df.select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Mortalidade_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-mortalidade\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Mortalidade_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-mortalidade\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                               [251]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"Mortalidade\", IntegerType())\n",
    "Mortalidade_df.where(\"timestamp >= '2021-11-10 13:52'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dataframe to a kafka topic\n",
    "(Letalidade_json.write\n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\",\"kafka:9092\") \n",
    "                        .option(\"topic\",\"topic-Letalidade\")                \n",
    "                        .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the topic saved to a temp dataframe and show the results\n",
    "Letalidade_df = (spark \n",
    "                        .read \n",
    "                        .format(\"kafka\") \n",
    "                        .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \n",
    "                        .option(\"subscribe\", \"topic-Letalidade\")  \n",
    "                        .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|jsontostructs(CAST(value AS STRING))|\n",
      "+------------------------------------+\n",
      "|                                 [5]|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the binary result in the dataframe to integer for visualization\n",
    "schema = StructType().add(\"Letalidade\", IntegerType())\n",
    "Letalidade_df.where(\"timestamp >= '2021-11-10 14:15'\").select(from_json(col(\"value\").cast(\"string\"), schema)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      regiao|   Casos|Obitos|Incidencia|Mortalidade|Atualizacao|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "|      Brasil|18855015| 99572|   8972.29|     250.73| 2021-07-06|\n",
      "|Centro-Oeste|  686433|  9980|    326.64|       9.27| 2021-07-06|\n",
      "|    Nordeste| 1141612|  9993|    543.24|      11.62| 2021-07-06|\n",
      "|       Norte|  557708|  9992|    265.39|       7.43| 2021-07-06|\n",
      "|     Sudeste| 3809222| 99989|   1812.65|      62.05| 2021-07-06|\n",
      "|         Sul| 1308643|   999|    622.73|      15.16| 2021-07-06|\n",
      "+------------+--------+------+----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.sql(\"\"\"select regiao, \n",
    "         max(casosAcumulado) as Casos, \n",
    "         max(obitosAcumulado) as Obitos, \n",
    "         max(cast(((casosAcumulado/210147125)*100000) as decimal(18,2))) as Incidencia, \n",
    "         max(cast(((obitosAcumulado/210147125)*100000) as decimal(18,2))) as Mortalidade, \n",
    "         max(data) as Atualizacao\n",
    "         from covid_br_data \n",
    "         group by regiao \n",
    "         order by regiao\"\"\")\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elastic View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a topic in elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o927.save.\n: java.lang.ClassNotFoundException: Failed to find data source: org.elasticsearch.spark.sql. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-16bd67410b72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es.port'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                             .option('es.resource', '%s/%s' % ('index_name', 'doc_type_name'),\n\u001b[0;32m----> 7\u001b[0;31m                             ).save(\"user/elastic\"))\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    734\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o927.save.\n: java.lang.ClassNotFoundException: Failed to find data source: org.elasticsearch.spark.sql. Please find packages at http://spark.apache.org/third-party-projects.html\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:657)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:244)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.DefaultSource\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20$$anonfun$apply$12.apply(DataSource.scala:634)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$20.apply(DataSource.scala:634)\n\tat scala.util.Try.orElse(Try.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:634)\n\t... 13 more\n"
     ]
    }
   ],
   "source": [
    "# Dataframe to Elastic - topic\n",
    "obitos_acumulados_es = (Obitos_Acumulados_json.write\n",
    "                            .format('org.elasticsearch.spark.sql')\n",
    "                            .option('es.nodes', 'localhost')\n",
    "                            .option('es.port', 9200)\n",
    "                            .option('es.resource', '%s/%s' % ('index_name', 'doc_type_name'),\n",
    "                            ).save(\"user/elastic\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
